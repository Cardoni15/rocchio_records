{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "from langdetect import detect, DetectorFactory\n",
    "import multiprocessing\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[K     |████████████████████████████████| 981 kB 784 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages (from langdetect) (1.15.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993221 sha256=44c6145145bacc1f13efb41076d3cd325f994e85efad37a17fd325419f1fafa5\n",
      "  Stored in directory: /Users/cardoni/Library/Caches/pip/wheels/13/c7/b0/79f66658626032e78fc1a83103690ef6797d551cb22e56e734\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 22.2.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# pip importer\n",
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#track_df = pd.read_csv()\n",
    "temp_path = os.getcwd()\n",
    "root_path = temp_path.split('/napster_2')[0]\n",
    "repo_path = '/napster_2/search_functionality/final_merged_data.csv'\n",
    "practice_data_path = root_path + repo_path\n",
    "\n",
    "og_path = '/napster_2/lyric_genius_api/practice_data.csv'\n",
    "og_path = root_path + og_path\n",
    "og_df = pd.read_csv(og_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(practice_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.shape\n",
    "# need track name, track id, artist name, raw lyrics\n",
    "test_df.columns\n",
    "track_df = test_df[['artist_name', 'track_name', 'track_id', 'raw_lyrics']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lyrics_data():\n",
    "    \"\"\"\n",
    "    return a dataframe with cleaned lyrics\n",
    "    \"\"\"\n",
    "    temp_path = os.getcwd()\n",
    "    root_path = temp_path.split('/napster_2')[0]\n",
    "    repo_path = '/napster_2/search_functionality/final_merged_data.csv'\n",
    "    data_path = root_path + repo_path\n",
    "    # step 1 import old lyrical data into a dataframe.\n",
    "    lyric_df = pd.read_csv(data_path)\n",
    "    lyric_df = lyric_df[['track_name', 'artist_name', 'track_id','raw_lyrics']]\n",
    "    # drop duplicate instances of the same track.\n",
    "    lyric_df = lyric_df.drop_duplicates(subset='raw_lyrics').reset_index(drop=True)\n",
    "    lyric_df = lyric_df.dropna()\n",
    "    # replace new line character\n",
    "    lyric_df['raw_lyrics'].replace('\\n', ' ',regex=True, inplace=True)\n",
    "    # remove embed text from lyric genius API\n",
    "    lyric_df['raw_lyrics'].replace('[0-9]{1,3}Embed', '', regex=True, inplace=True)\n",
    "    return lyric_df\n",
    "\n",
    "test_df = load_lyrics_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84502/84502 [00:00<00:00, 534369.64it/s]\n"
     ]
    }
   ],
   "source": [
    "test_text = test_df['raw_lyrics'][0]\n",
    "DetectorFactory.seed = 0\n",
    "detect(test_text)\n",
    "lang_list = []\n",
    "for track in tqdm(test_df['raw_lyrics']):\n",
    "    #temp_lang = detect(track)\n",
    "    #lang_list.append(temp_lang)\n",
    "    pass\n",
    "\n",
    "#Counter(lang_list)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to remove tracks that are not in english\n",
    "# interesting side application can we find the most similar track to a given track IN a different language?\n",
    "test_df['language'] = test_df['raw_lyrics'].apply(detect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df[test_df.language == 'en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# steps for Rocchio Feedback Filter\n",
    "# PROCESS 1 convert the raw lyrics into the concept space.\n",
    "# 1. Create a TFIDF vectorizer\n",
    "# 2. Create a document term matrix using TFIDF vec fit_transform using the raw lyrics. [I think there is an option to lemmatize here]\n",
    "# 3. Complete latent semantic indexing using TruncatedSVD(num components = num comncepts, specifiy the random state)\n",
    "# 4. Fit the document term matrix using TruncatedSVD.fit_transform. THESE ARE YOUR VECTORS FOR SIMILARITY SCORING\n",
    "\n",
    "# PROCESS 2 convert the query into a vector\n",
    "# 1. Convert querry into a raw string\n",
    "# 2. Use the TFIDF vectorizer above to transform the querry\n",
    "# 3. Use the LSI object above to convert the querry into the concept space.\n",
    "\n",
    "# PROCESS 3 execute the search\n",
    "# 1. Find the cosine similarity between the querry and all lyrics\n",
    "# 2. Sort the tracks by similarity\n",
    "# 3. Return the top N tracks to the user.\n",
    "\n",
    "# PROCESS 4 Rochhio Feedback Filtering\n",
    "# 1. Group user feeback by love, no answer, dislike\n",
    "# 2. Calculate the mean for each group\n",
    "# 3. Apply alpha, beta, gamma, and phi to:\n",
    "#       Original search, loves, hates, nuetral\n",
    "# 4. Update the lyric search querry vector and return new results!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['track_name_x', 'artist_name_x', 'lyric_raw'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/cardoni/napster_2/search_functionality/development.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cardoni/napster_2/search_functionality/development.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# drop latin songs to avoid negatively impacting performance using 2 languages\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cardoni/napster_2/search_functionality/development.ipynb#X14sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m lyric_df \u001b[39m=\u001b[39m lyric_df[lyric_df[\u001b[39m'\u001b[39m\u001b[39mgenre\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mlatin\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/cardoni/napster_2/search_functionality/development.ipynb#X14sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m lyric_df \u001b[39m=\u001b[39m lyric_df[[\u001b[39m'\u001b[39;49m\u001b[39mtrack_name_x\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39martist_name_x\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mtrack_id\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mlyric_raw\u001b[39;49m\u001b[39m'\u001b[39;49m]]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cardoni/napster_2/search_functionality/development.ipynb#X14sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m lyric_df\u001b[39m.\u001b[39mrename(columns\u001b[39m=\u001b[39m{\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cardoni/napster_2/search_functionality/development.ipynb#X14sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrack_name_x\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mtrack_name\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cardoni/napster_2/search_functionality/development.ipynb#X14sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39martist_name_x\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39martist_name\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cardoni/napster_2/search_functionality/development.ipynb#X14sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m }, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cardoni/napster_2/search_functionality/development.ipynb#X14sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m lyric_df \u001b[39m=\u001b[39m lyric_df\u001b[39m.\u001b[39mdropna()\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pandas/core/frame.py:3511\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3509\u001b[0m     \u001b[39mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3510\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[0;32m-> 3511\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49m_get_indexer_strict(key, \u001b[39m\"\u001b[39;49m\u001b[39mcolumns\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m1\u001b[39m]\n\u001b[1;32m   3513\u001b[0m \u001b[39m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3514\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(indexer, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39mbool\u001b[39m:\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pandas/core/indexes/base.py:5782\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   5779\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   5780\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 5782\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   5784\u001b[0m keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(indexer)\n\u001b[1;32m   5785\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Index):\n\u001b[1;32m   5786\u001b[0m     \u001b[39m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pandas/core/indexes/base.py:5845\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   5842\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of [\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m] are in the [\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   5844\u001b[0m not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39munique())\n\u001b[0;32m-> 5845\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['track_name_x', 'artist_name_x', 'lyric_raw'] not in index\""
     ]
    }
   ],
   "source": [
    "# Tuning Parameters\n",
    "# minimum document frequency\n",
    "min_df = 10\n",
    "num_concepts = 100\n",
    "# step 1 import old lyrical data into a dataframe.\n",
    "lyric_df = pd.read_csv(practice_data_path)\n",
    "# drop latin songs to avoid negatively impacting performance using 2 languages\n",
    "lyric_df = lyric_df[lyric_df['genre'] != 'latin']\n",
    "lyric_df = lyric_df[['track_name_x', 'artist_name_x', 'track_id','lyric_raw']]\n",
    "lyric_df.rename(columns={\n",
    "    'track_name_x': 'track_name',\n",
    "    'artist_name_x': 'artist_name'\n",
    "}, inplace=True)\n",
    "lyric_df = lyric_df.dropna()\n",
    "# replace new line character\n",
    "lyric_df['lyric_raw'].replace('\\n', ' ',regex=True, inplace=True)\n",
    "# remove embed text from lyric genius API\n",
    "lyric_df['lyric_raw'].replace('[0-9]{1,3}Embed', '', regex=True, inplace=True)\n",
    "# create vectorizer object\n",
    "vecObj = TfidfVectorizer(tokenizer=str.split, min_df=min_df)\n",
    "# fit the TFIDF vectorizer\n",
    "docTermMat = vecObj.fit_transform(lyric_df['lyric_raw'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'docTermMat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/cardoni/napster_2/search_functionality/development.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cardoni/napster_2/search_functionality/development.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m lsiObj \u001b[39m=\u001b[39m TruncatedSVD(n_components\u001b[39m=\u001b[39mnum_concepts, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/cardoni/napster_2/search_functionality/development.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m docVecs \u001b[39m=\u001b[39m lsiObj\u001b[39m.\u001b[39mfit_transform(docTermMat)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cardoni/napster_2/search_functionality/development.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# create a dataframe where the track id is the index the docVecs are the rows.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cardoni/napster_2/search_functionality/development.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m track_vec_dict \u001b[39m=\u001b[39m defaultdict(\u001b[39mlist\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'docTermMat' is not defined"
     ]
    }
   ],
   "source": [
    "lsiObj = TruncatedSVD(n_components=num_concepts, random_state=42)\n",
    "docVecs = lsiObj.fit_transform(docTermMat)\n",
    "# create a dataframe where the track id is the index the docVecs are the rows.\n",
    "track_vec_dict = defaultdict(list)\n",
    "track_ids = lyric_df['track_id'].values\n",
    "track_vec_dict = {track_ids[i]: docVecs[i] for i in range(len(track_ids))}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vecObj' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/cardoni/napster_2/search_functionality/development.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cardoni/napster_2/search_functionality/development.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m user_query \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mJealousy, turning saints into the sea Swimming through sick lullabies Choking on your alibis\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cardoni/napster_2/search_functionality/development.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# vectorize\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/cardoni/napster_2/search_functionality/development.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m userVec \u001b[39m=\u001b[39m vecObj\u001b[39m.\u001b[39mtransform([user_query])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cardoni/napster_2/search_functionality/development.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# convert query vec into the concept space\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cardoni/napster_2/search_functionality/development.ipynb#X16sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m userLsi \u001b[39m=\u001b[39m lsiObj\u001b[39m.\u001b[39mtransform(userVec)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vecObj' is not defined"
     ]
    }
   ],
   "source": [
    "# part 2 convert the query into a vector in the concept space\n",
    "user_query = 'Jealousy, turning saints into the sea Swimming through sick lullabies Choking on your alibis'\n",
    "# vectorize\n",
    "userVec = vecObj.transform([user_query])\n",
    "# convert query vec into the concept space\n",
    "userLsi = lsiObj.transform(userVec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part 3 execute search using cosine similarity\n",
    "# 1. Find the cosine similarity between the query and all lyrics\n",
    "# 2. Sort the tracks by similarity\n",
    "# 3. Return the top N tracks to the user.\n",
    "\n",
    "# calculate cosine similarity between every track and the lyric provided.\n",
    "simVals = cosine_similarity(docVecs, userLsi)\n",
    "# create a track name, track id, artist name, similarity dataframe\n",
    "lyric_df['similarity'] = simVals\n",
    "\n",
    "# this step is important, \n",
    "# the lyrics df is officially out of sync now, \n",
    "# the indexes need to be sorted again OR two copies need to be maintained\n",
    "sim_df = lyric_df.sort_values(by='similarity', ascending=False)\n",
    "user_playlist = sim_df.head(30)[['track_name', 'artist_name', 'track_id']]\n",
    "# initialize a feedback column and set every row to 0.\n",
    "user_playlist['feedback'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate user input here\n",
    "feedback = [np.random.randint(0,3) for i in range(len(user_playlist))]\n",
    "user_playlist['feedback'] = feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROCESS 4 Rochhio Feedback Filtering\n",
    "# 1. Group user feeback by love, nuetral, dislike\n",
    "# 2. Calculate the mean for each group\n",
    "# 3. Apply alpha, beta, gamma, and phi to:\n",
    "#       Original search, loves, hates, nuetral\n",
    "# 4. Update the lyric search querry vector and return new results!\n",
    "\n",
    "# tuning parameters\n",
    "# original querry gets no penalty\n",
    "alpha = 1.0\n",
    "# loved songs get a beta positive weight.\n",
    "beta = 0.75\n",
    "# disliked songs get a gamma negative weight\n",
    "gamma = 0.25\n",
    "# nuetral songs get a phi positive weight\n",
    "phi = 0.5\n",
    "\n",
    "# create a mean vector dict for all 3 states\n",
    "meanVectDict = defaultdict(list)\n",
    "\n",
    "# iterate through the three states nuetral[0], dislike[1], love[2]\n",
    "for i in range(3):\n",
    "    temp_tracks = user_playlist[user_playlist['feedback']==i]\n",
    "    if len(temp_tracks) > 0:\n",
    "        # this means that tracks with this sentiment exist.\n",
    "        # we can go get the track vectors from the track_vec_dict\n",
    "        tempVecs = [track_vec_dict[vec] for vec in temp_tracks['track_id']]\n",
    "        # next we need to calculate the mean vector for this segment\n",
    "        meanVec = np.mean(tempVecs, axis=0)\n",
    "        # add this mean to the mean vect dict. The key is the state.\n",
    "        meanVectDict[i] = meanVec\n",
    "    else:\n",
    "        # if there are no tracks in this state, set its mean to 0\n",
    "        meanVectDict[i] = 0\n",
    "\n",
    "# calcualte the new query vector by summing all of the mean vectors together\n",
    "newQueryVec = alpha*userLsi + beta * meanVectDict[2] - gamma * meanVectDict[1] + phi * meanVectDict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5 convert the notebook into functions.\n",
    "def load_lyrics_data():\n",
    "    \"\"\"\n",
    "    return a dataframe with cleaned lyrics\n",
    "    \"\"\"\n",
    "    temp_path = os.getcwd()\n",
    "    root_path = temp_path.split('/napster_2')[0]\n",
    "    repo_path = '/napster_2/lyric_genius_api/practice_data.csv'\n",
    "    data_path = root_path + repo_path\n",
    "    # step 1 import old lyrical data into a dataframe.\n",
    "    lyric_df = pd.read_csv(data_path)\n",
    "    # drop latin songs to avoid negatively impacting performance using 2 languages\n",
    "    lyric_df = lyric_df[lyric_df['genre'] != 'latin']\n",
    "    lyric_df = lyric_df[['track_name_x', 'artist_name_x', 'track_id','lyric_raw']]\n",
    "    lyric_df.rename(columns={\n",
    "        'track_name_x': 'track_name',\n",
    "        'artist_name_x': 'artist_name'\n",
    "    }, inplace=True)\n",
    "    lyric_df = lyric_df.dropna()\n",
    "    # replace new line character\n",
    "    lyric_df['lyric_raw'].replace('\\n', ' ',regex=True, inplace=True)\n",
    "    # remove embed text from lyric genius API\n",
    "    lyric_df['lyric_raw'].replace('[0-9]{1,3}Embed', '', regex=True, inplace=True)\n",
    "    return lyric_df\n",
    "\n",
    "def create_lyric_tfidf(lyric_df, min_df):\n",
    "    \"\"\" \n",
    "    Create a tfidf vectorizer for the track lyrics\n",
    "    \"\"\"\n",
    "    tfidf = TfidfVectorizer(tokenizer=str.split, min_df=min_df)\n",
    "    # fit the TFIDF vectorizer\n",
    "    tfidf.fit(lyric_df['raw_lyrics'])\n",
    "    return tfidf\n",
    "\n",
    "def lsi_lyrics(lyric_df, tfidf, num_concepts):\n",
    "    \"\"\"\n",
    "    fit an LSI object using the lyrics\n",
    "    \"\"\"\n",
    "    lyricTermMat = tfidf.transform(lyric_df['raw_lyrics'])\n",
    "    lsiObj = TruncatedSVD(n_components=num_concepts, random_state=15)\n",
    "    lsiObj.fit(lyricTermMat)\n",
    "    return lsiObj\n",
    "\n",
    "def create_lyric_vecs(lyric_df, lsiObj, tfidf):\n",
    "    \"\"\" \n",
    "    generate the lyric vectors in the concept space\n",
    "    \"\"\"\n",
    "    lyricTermMat = tfidf.transform(lyric_df['raw_lyrics'])\n",
    "    lyric_vecs = lsiObj.transform(lyricTermMat)\n",
    "    return lyric_vecs\n",
    "\n",
    "def create_lyric_dictionary(lyric_df, lyric_vecs):\n",
    "    \"\"\" \n",
    "    input a lyric dataframe and lsi lyric vectors\n",
    "    return a dictionary where the track id is the key\n",
    "    the vector is the value\n",
    "    \"\"\"\n",
    "    # create a dataframe where the track id is the index the docVecs are the rows.\n",
    "    track_vec_dict = defaultdict(list)\n",
    "    track_ids = lyric_df['track_id'].values\n",
    "    track_vec_dict = {track_ids[i]: lyric_vecs[i] for i in range(len(track_ids))}\n",
    "    return track_vec_dict\n",
    "\n",
    "def lsi_on_query(lsiObj, user_query, tfidf):\n",
    "    \"\"\" \n",
    "    Transform the user search string into the concept space\n",
    "    \"\"\"\n",
    "    userVec = tfidf.transform([user_query])\n",
    "    # convert query vec into the concept space\n",
    "    userLsi = lsiObj.transform(userVec)\n",
    "    return userLsi\n",
    "\n",
    "def retreive_30_tracks(lyric_df, userLsi, lyric_vecs):\n",
    "    \"\"\" \n",
    "    calculate cosine similarity between lyrics and user query\n",
    "    return the top 30 in a dataframe.\n",
    "    \"\"\"\n",
    "    # calculate cosine similarity between every track and the lyric provided.\n",
    "    simVals = cosine_similarity(lyric_vecs, userLsi)\n",
    "    # create a track name, track id, artist name, similarity dataframe\n",
    "    # need to make this a copy so that we do not modify the already stored data\n",
    "    lyric_df['similarity'] = simVals\n",
    "    user_playlist = lyric_df.sort_values(by='similarity', ascending=False)\n",
    "    user_playlist = user_playlist.head(30)[['track_name', 'artist_name', 'track_id']]\n",
    "    # initialize a feedback column and set every row to 0.\n",
    "    user_playlist['feedback'] = 0\n",
    "    return user_playlist\n",
    "\n",
    "def simulate_user_input(user_playlist):\n",
    "    \"\"\" \n",
    "    Simulate user input assign 0,1,2 to user feedback\n",
    "    0 = nuetral\n",
    "    1 = dislike\n",
    "    2 = love\n",
    "    \"\"\"\n",
    "    feedback = [np.random.randint(0,3) for i in range(len(user_playlist))]\n",
    "    user_playlist['feedback'] = feedback\n",
    "    return user_playlist\n",
    "\n",
    "def rocchio_feedback(alpha, beta, gamma, phi, user_playlist, track_vec_dict):\n",
    "    \"\"\" \n",
    "    Rochhio Feedback Filtering\n",
    "    1. Group user feeback by love, nuetral, dislike\n",
    "    2. Calculate the mean for each group\n",
    "    3. Apply alpha, beta, gamma, and phi to:\n",
    "           Original search, loves, hates, nuetral\n",
    "    4. Update the lyric search querry vector and return new results!\n",
    "    return an updated query vector to improve search results\n",
    "    \"\"\"\n",
    "    # create a mean vector dict for all 3 states\n",
    "    meanVectDict = defaultdict(list)\n",
    "    # iterate through the three states nuetral[0], dislike[1], love[2]\n",
    "    for i in range(3):\n",
    "        temp_tracks = user_playlist[user_playlist['feedback']==i]\n",
    "        if len(temp_tracks) > 0:\n",
    "            # this means that tracks with this sentiment exist.\n",
    "            # we can go get the track vectors from the track_vec_dict\n",
    "            tempVecs = [track_vec_dict[vec] for vec in temp_tracks['track_id']]\n",
    "            # next we need to calculate the mean vector for this segment\n",
    "            meanVec = np.mean(tempVecs, axis=0)\n",
    "            # add this mean to the mean vect dict. The key is the state.\n",
    "            meanVectDict[i] = meanVec\n",
    "        else:\n",
    "            # if there are no tracks in this state, set its mean to 0\n",
    "            meanVectDict[i] = 0\n",
    "    # calcualte the new query vector by summing all of the mean vectors together\n",
    "    newQueryVec = alpha*userLsi + beta * meanVectDict[2] - gamma * meanVectDict[1] + phi * meanVectDict[0]\n",
    "    return newQueryVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cardoni/Library/Python/3.8/lib/python/site-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 6 integrate all functions into a loop\n",
    "user_query =  'Jealousy, turning saints into the sea Swimming through sick lullabies Choking on your alibis'\n",
    "# load in lyrics dataframe\n",
    "lyric_df = test_df #load_lyrics_data()\n",
    "# create a tfidf vectorizer object\n",
    "tfidf = create_lyric_tfidf(lyric_df, 10)\n",
    "# create a latent semantic indexing object\n",
    "lsiObj = lsi_lyrics(lyric_df, tfidf, 100)\n",
    "# convert the lyrics into content vectors\n",
    "lyric_vecs = create_lyric_vecs(lyric_df, lsiObj, tfidf)\n",
    "# create a dictionary mapping track id to content vector\n",
    "track_vec_dict = create_lyric_dictionary(lyric_df, lyric_vecs)\n",
    "# convert a user string query into the concept space\n",
    "userLsi = lsi_on_query(lsiObj, user_query, tfidf)\n",
    "# create a user playlist and return a dataframe\n",
    "user_playlist = retreive_30_tracks(lyric_df, userLsi, lyric_vecs)\n",
    "# simulate user input while we are not connected to the GUI\n",
    "user_playlist = simulate_user_input(user_playlist)\n",
    "# Apply rocchio feedback filter to generate a better query\n",
    "rocchioSearch = rocchio_feedback(1.0, 0.75, 0.25, 0.5, user_playlist, track_vec_dict)\n",
    "# generate a new user playlist with the updated search\n",
    "user_playlist = retreive_30_tracks(lyric_df, rocchioSearch, lyric_vecs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(track_vec_dict, open('lsi_vec_dict.p', 'wb'))\n",
    "#pickle.dump(test_df, open('clean_english_tracks_df.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.shape\n",
    "track_artist_id_df = test_df[['track_name', 'artist_name', 'track_id']].drop_duplicates()\n",
    "track_artist_id_df.to_csv('track_artist_id_df', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lsi_dict_pickle(track_vec_dict):\n",
    "    pickle.dump(track_vec_dict, open('lsi_vec_dict.p', 'wb'))\n",
    "def create_lsi_obj_pickle(lsiObj):\n",
    "    pickle.dump(lsiObj, open('lsi_obj.p', 'wb'))\n",
    "def create_tfidf_obj_pickle(tfidf):\n",
    "    pickle.dump(tfidf, open('tfidf_obj.p', 'wb'))\n",
    "def load_lsi_pickle():\n",
    "    \"\"\" \n",
    "    read in the pickle file containing the fitted\n",
    "    LSI object\n",
    "    \"\"\"\n",
    "    with open('lsi_obj.p', 'rb') as lsi_file:\n",
    "        lsiObj = pickle.load(lsi_file)\n",
    "        return lsiObj\n",
    "def load_tfidf_pickle():\n",
    "    \"\"\" \n",
    "    read in the pickle file containing the fitted\n",
    "    TFIDF object\n",
    "    \"\"\"\n",
    "    with open('tfidf_obj.p', 'rb') as tfidf_file:\n",
    "        tfidf = pickle.load(tfidf_file)\n",
    "        return tfidf\n",
    "def load_lsi_dict_pickle():\n",
    "    \"\"\" \n",
    "    read in the pickle file containing the fitted\n",
    "    TFIDF object\n",
    "    \"\"\"\n",
    "    with open('lsi_vec_dict.p', 'rb') as tfidf_file:\n",
    "        tfidf = pickle.load(tfidf_file)\n",
    "        return tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_series = pd.Series(['like', 'dislike', 'nuetral'])\n",
    "feedback_series.replace({\n",
    "            'nuetral' : 0,\n",
    "            'dislike' : 1,\n",
    "            'like' : 2\n",
    "        }, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_lsi_dict_pickle(track_vec_dict)\n",
    "create_lsi_obj_pickle(lsiObj)\n",
    "create_tfidf_obj_pickle(tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cardoni/Library/Python/3.8/lib/python/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator TruncatedSVD from version 1.0.2 when using version 1.1.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/cardoni/Library/Python/3.8/lib/python/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator TfidfTransformer from version 1.0.2 when using version 1.1.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/cardoni/Library/Python/3.8/lib/python/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 1.0.2 when using version 1.1.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'is this thing on'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rocchio_filter import Napster2_Rocchio_Feedback\n",
    "import pandas as pd\n",
    "rff = Napster2_Rocchio_Feedback()\n",
    "rff.userSearch('Jealousy turning saints into the sea')\n",
    "rff.create_user_playlist()\n",
    "track_df = rff.return_top_10_tracks()\n",
    "feedback = pd.Series(['Like', 'Dislike','Dislike', 'Dislike','Dislike', 'Dislike','Dislike', 'Dislike','Dislike', 'Dislike'])                                          \n",
    "rff.apply_feedback(feedback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_name</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>track_id</th>\n",
       "      <th>feedback</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kelly Watch the Stars</td>\n",
       "      <td>Air</td>\n",
       "      <td>1KNCpsUEi6VdDIBzZ1cMVd</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mayhem</td>\n",
       "      <td>Steve Aoki</td>\n",
       "      <td>0Xz3DTcdZI01mjKfNZrsC1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Boss</td>\n",
       "      <td>Pete Rock</td>\n",
       "      <td>4Qb7yDdOIpbe3N6X8eYf5A</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Split The Atom</td>\n",
       "      <td>Noisia</td>\n",
       "      <td>6K2XPbZ4maj8ud5EeKsYIn</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Nightfly - Short Cut</td>\n",
       "      <td>Blank &amp; Jones</td>\n",
       "      <td>4sp4gj3TRXsBKppvXZkTmj</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Not Unlike The Waves</td>\n",
       "      <td>Agalloch</td>\n",
       "      <td>57brEAGObTsuJnzMxd8F0G</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Frankie And Johnny</td>\n",
       "      <td>Mississippi John Hurt</td>\n",
       "      <td>3WeWCiniWbZD4FCPuy3qQu</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Élan</td>\n",
       "      <td>Nightwish</td>\n",
       "      <td>6mUJRMe2bZdn8EOnBzKeix</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Self-Inflicted Mental Terror</td>\n",
       "      <td>Gulch</td>\n",
       "      <td>5vXGOHMUAk3OkSJjz2UDGo</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Surface</td>\n",
       "      <td>Aero Chord</td>\n",
       "      <td>63bR379dNknfAws2hBBsq7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     track_name            artist_name  \\\n",
       "0         Kelly Watch the Stars                    Air   \n",
       "1                        Mayhem             Steve Aoki   \n",
       "2                      The Boss              Pete Rock   \n",
       "3                Split The Atom                 Noisia   \n",
       "4      The Nightfly - Short Cut          Blank & Jones   \n",
       "5          Not Unlike The Waves               Agalloch   \n",
       "6            Frankie And Johnny  Mississippi John Hurt   \n",
       "7                          Élan              Nightwish   \n",
       "8  Self-Inflicted Mental Terror                  Gulch   \n",
       "9                       Surface             Aero Chord   \n",
       "\n",
       "                 track_id  feedback  \n",
       "0  1KNCpsUEi6VdDIBzZ1cMVd         2  \n",
       "1  0Xz3DTcdZI01mjKfNZrsC1         1  \n",
       "2  4Qb7yDdOIpbe3N6X8eYf5A         1  \n",
       "3  6K2XPbZ4maj8ud5EeKsYIn         1  \n",
       "4  4sp4gj3TRXsBKppvXZkTmj         1  \n",
       "5  57brEAGObTsuJnzMxd8F0G         1  \n",
       "6  3WeWCiniWbZD4FCPuy3qQu         1  \n",
       "7  6mUJRMe2bZdn8EOnBzKeix         1  \n",
       "8  5vXGOHMUAk3OkSJjz2UDGo         1  \n",
       "9  63bR379dNknfAws2hBBsq7         1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rff.user_playlist"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
