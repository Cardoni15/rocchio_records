{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict, Counter\n",
    "from langdetect import detect, DetectorFactory\n",
    "import multiprocessing\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is used to create Latent Semantic Indexing objects, TFIDF Vectorizers, and lyric cleaning.\n",
    "This notebook creates pickle files that the application uses.\n",
    "\n",
    "Pickle files are used to save runtime during search. The pickle files are pre-generated allowing us to go directly into using them for search.\n",
    "This also allows us to store the concept vectors instead of the full lyrics greatly reducing memory usage.\n",
    "\n",
    "NOTE: it is not recommended to change the pickle files saved unless you have new track data that will replace the original pickle files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for generating LSI, TFIDF and Lyic Vectors.\n",
    "def load_lyrics_data():\n",
    "    \"\"\"\n",
    "    return a dataframe with cleaned lyrics\n",
    "    \"\"\"\n",
    "    temp_path = os.getcwd()\n",
    "    root_path = temp_path.split('/rocchio_records')[0]\n",
    "    repo_path = '/rocchio_records/lyric_genius_api/practice_data.csv'\n",
    "    data_path = root_path + repo_path\n",
    "    # step 1 import old lyrical data into a dataframe.\n",
    "    lyric_df = pd.read_csv(data_path)\n",
    "    lyric_df = lyric_df[['track_name', 'artist_name', 'track_id','raw_lyrics']]\n",
    "    # drop duplicate instances of the same track.\n",
    "    lyric_df = lyric_df.drop_duplicates(subset='raw_lyrics').reset_index(drop=True)\n",
    "    lyric_df = lyric_df.dropna()\n",
    "    # replace new line character\n",
    "    lyric_df['raw_lyrics'].replace('\\n', ' ',regex=True, inplace=True)\n",
    "    # remove embed text from lyric genius API\n",
    "    lyric_df['raw_lyrics'].replace('[0-9]{1,3}Embed', '', regex=True, inplace=True)\n",
    "    return lyric_df\n",
    "\n",
    "def remove_non_english_tracks(df):\n",
    "    \"\"\" \n",
    "    Detect the language of a track\n",
    "    Drop english tracks\n",
    "    Return a DF with all english tracks\n",
    "    \"\"\"\n",
    "    df['language'] = df['raw_lyrics'].apply(detect)\n",
    "    df = df[df.language == 'en']\n",
    "    return df\n",
    "\n",
    "def create_lyric_tfidf(lyric_df, min_df):\n",
    "    \"\"\" \n",
    "    Create a tfidf vectorizer for the track lyrics\n",
    "    \"\"\"\n",
    "    tfidf = TfidfVectorizer(tokenizer=str.split, min_df=min_df)\n",
    "    # fit the TFIDF vectorizer\n",
    "    tfidf.fit(lyric_df['raw_lyrics'])\n",
    "    return tfidf\n",
    "\n",
    "def lsi_lyrics(lyric_df, tfidf, num_concepts):\n",
    "    \"\"\"\n",
    "    fit an LSI object using the lyrics\n",
    "    \"\"\"\n",
    "    lyricTermMat = tfidf.transform(lyric_df['raw_lyrics'])\n",
    "    lsiObj = TruncatedSVD(n_components=num_concepts, random_state=15)\n",
    "    lsiObj.fit(lyricTermMat)\n",
    "    return lsiObj\n",
    "\n",
    "def create_lyric_vecs(lyric_df, lsiObj, tfidf):\n",
    "    \"\"\" \n",
    "    generate the lyric vectors in the concept space\n",
    "    \"\"\"\n",
    "    lyricTermMat = tfidf.transform(lyric_df['raw_lyrics'])\n",
    "    lyric_vecs = lsiObj.transform(lyricTermMat)\n",
    "    return lyric_vecs\n",
    "\n",
    "def create_lyric_dictionary(lyric_df, lyric_vecs):\n",
    "    \"\"\" \n",
    "    input a lyric dataframe and lsi lyric vectors\n",
    "    return a dictionary where the track id is the key\n",
    "    the vector is the value\n",
    "    \"\"\"\n",
    "    # create a dataframe where the track id is the index the docVecs are the rows.\n",
    "    track_vec_dict = defaultdict(list)\n",
    "    track_ids = lyric_df['track_id'].values\n",
    "    track_vec_dict = {track_ids[i]: lyric_vecs[i] for i in range(len(track_ids))}\n",
    "    return track_vec_dict\n",
    "\n",
    "def simulate_user_input(user_playlist):\n",
    "    \"\"\" \n",
    "    Simulate user input assign 0,1,2 to user feedback\n",
    "    0 = nuetral\n",
    "    1 = dislike\n",
    "    2 = love\n",
    "    \"\"\"\n",
    "    feedback = [np.random.randint(0,3) for i in range(len(user_playlist))]\n",
    "    user_playlist['feedback'] = feedback\n",
    "    return user_playlist\n",
    "\n",
    "\n",
    "# WARNING USING THESE FUNCTIONS CAN OVER WRITE THE ORIGINAL FILES THE APPLICATION USES\n",
    "# ONLY USES THESE FUNCTIONS IF YOU HAVE NEW TRAINING DATA\n",
    "def create_artist_track_id_df(test_df):\n",
    "    \"\"\"\n",
    "    Write a DF to csv for running in the application.\n",
    "    \"\"\"\n",
    "    track_artist_id_df = test_df[['track_name', 'artist_name', 'track_id']].drop_duplicates()\n",
    "    track_artist_id_df.to_csv('track_artist_id_df', index=False)\n",
    "def create_lsi_dict_pickle(track_vec_dict):\n",
    "    pickle.dump(track_vec_dict, open('lsi_vec_dict.p', 'wb'))\n",
    "def create_lsi_obj_pickle(lsiObj):\n",
    "    pickle.dump(lsiObj, open('lsi_obj.p', 'wb'))\n",
    "def create_tfidf_obj_pickle(tfidf):\n",
    "    pickle.dump(tfidf, open('tfidf_obj.p', 'wb'))\n",
    "def load_lsi_pickle():\n",
    "    \"\"\" \n",
    "    read in the pickle file containing the fitted\n",
    "    LSI object\n",
    "    \"\"\"\n",
    "    with open('lsi_obj.p', 'rb') as lsi_file:\n",
    "        lsiObj = pickle.load(lsi_file)\n",
    "        return lsiObj\n",
    "def load_tfidf_pickle():\n",
    "    \"\"\" \n",
    "    read in the pickle file containing the fitted\n",
    "    TFIDF object\n",
    "    \"\"\"\n",
    "    with open('tfidf_obj.p', 'rb') as tfidf_file:\n",
    "        tfidf = pickle.load(tfidf_file)\n",
    "        return tfidf\n",
    "def load_lsi_dict_pickle():\n",
    "    \"\"\" \n",
    "    read in the pickle file containing the fitted\n",
    "    TFIDF object\n",
    "    \"\"\"\n",
    "    with open('lsi_vec_dict.p', 'rb') as tfidf_file:\n",
    "        tfidf = pickle.load(tfidf_file)\n",
    "        return tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cardoni/napster_2/venv3913/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:524: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# call the above functions to show how the LSI vector generation works.\n",
    "# load in lyrics dataframe\n",
    "lyric_df = load_lyrics_data()\n",
    "# remove non english tracks:\n",
    "lyric_df = remove_non_english_tracks(lyric_df)\n",
    "# create a tfidf vectorizer object\n",
    "tfidf = create_lyric_tfidf(lyric_df, 10)\n",
    "# create a latent semantic indexing object\n",
    "lsiObj = lsi_lyrics(lyric_df, tfidf, 100)\n",
    "# convert the lyrics into content vectors\n",
    "lyric_vecs = create_lyric_vecs(lyric_df, lsiObj, tfidf)\n",
    "# create a dictionary mapping track id to content vector\n",
    "track_vec_dict = create_lyric_dictionary(lyric_df, lyric_vecs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('venv3913': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0df20829181e44dd98a6a93cb10785e950889c363fb12790a41ecc840703d20d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
