{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "from langdetect import detect, DetectorFactory\n",
    "import multiprocessing\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[K     |████████████████████████████████| 981 kB 784 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages (from langdetect) (1.15.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993221 sha256=44c6145145bacc1f13efb41076d3cd325f994e85efad37a17fd325419f1fafa5\n",
      "  Stored in directory: /Users/cardoni/Library/Caches/pip/wheels/13/c7/b0/79f66658626032e78fc1a83103690ef6797d551cb22e56e734\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 22.2.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# pip importer\n",
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#track_df = pd.read_csv()\n",
    "temp_path = os.getcwd()\n",
    "root_path = temp_path.split('/napster_2')[0]\n",
    "repo_path = '/napster_2/search_functionality/final_merged_data.csv'\n",
    "practice_data_path = root_path + repo_path\n",
    "\n",
    "og_path = '/napster_2/lyric_genius_api/practice_data.csv'\n",
    "og_path = root_path + og_path\n",
    "og_df = pd.read_csv(og_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(practice_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.shape\n",
    "# need track name, track id, artist name, raw lyrics\n",
    "test_df.columns\n",
    "track_df = test_df[['artist_name', 'track_name', 'track_id', 'raw_lyrics']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lyrics_data():\n",
    "    \"\"\"\n",
    "    return a dataframe with cleaned lyrics\n",
    "    \"\"\"\n",
    "    temp_path = os.getcwd()\n",
    "    root_path = temp_path.split('/napster_2')[0]\n",
    "    repo_path = '/napster_2/search_functionality/final_merged_data.csv'\n",
    "    data_path = root_path + repo_path\n",
    "    # step 1 import old lyrical data into a dataframe.\n",
    "    lyric_df = pd.read_csv(data_path)\n",
    "    lyric_df = lyric_df[['track_name', 'artist_name', 'track_id','raw_lyrics']]\n",
    "    # drop duplicate instances of the same track.\n",
    "    lyric_df = lyric_df.drop_duplicates(subset='raw_lyrics').reset_index(drop=True)\n",
    "    lyric_df = lyric_df.dropna()\n",
    "    # replace new line character\n",
    "    lyric_df['raw_lyrics'].replace('\\n', ' ',regex=True, inplace=True)\n",
    "    # remove embed text from lyric genius API\n",
    "    lyric_df['raw_lyrics'].replace('[0-9]{1,3}Embed', '', regex=True, inplace=True)\n",
    "    return lyric_df\n",
    "\n",
    "test_df = load_lyrics_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84502/84502 [12:02<00:00, 116.93it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'en': 75976,\n",
       "         'pt': 238,\n",
       "         'hr': 811,\n",
       "         'tl': 291,\n",
       "         'es': 1257,\n",
       "         'sv': 1017,\n",
       "         'de': 975,\n",
       "         'nl': 599,\n",
       "         'ca': 54,\n",
       "         'mk': 14,\n",
       "         'fr': 1210,\n",
       "         'id': 112,\n",
       "         'no': 314,\n",
       "         'so': 67,\n",
       "         'af': 26,\n",
       "         'it': 127,\n",
       "         'sl': 23,\n",
       "         'ja': 198,\n",
       "         'fi': 386,\n",
       "         'tr': 65,\n",
       "         'sq': 53,\n",
       "         'cy': 22,\n",
       "         'cs': 9,\n",
       "         'sw': 74,\n",
       "         'sk': 10,\n",
       "         'da': 351,\n",
       "         'zh-cn': 4,\n",
       "         'lv': 4,\n",
       "         'ru': 50,\n",
       "         'pl': 47,\n",
       "         'el': 4,\n",
       "         'ro': 13,\n",
       "         'et': 11,\n",
       "         'hu': 12,\n",
       "         'ko': 54,\n",
       "         'ar': 1,\n",
       "         'lt': 1,\n",
       "         'he': 3,\n",
       "         'vi': 4,\n",
       "         'th': 8,\n",
       "         'uk': 5,\n",
       "         'hi': 1,\n",
       "         'bg': 1})"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text = test_df['raw_lyrics'][0]\n",
    "DetectorFactory.seed = 0\n",
    "detect(test_text)\n",
    "lang_list = []\n",
    "for track in tqdm(test_df['raw_lyrics']):\n",
    "    temp_lang = detect(track)\n",
    "    lang_list.append(temp_lang)\n",
    "\n",
    "Counter(lang_list)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to remove tracks that are not in english\n",
    "# interesting side application can we find the most similar track to a given track IN a different language?\n",
    "test_df['language'] = test_df['raw_lyrics'].apply(detect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df[test_df.language == 'en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# steps for Rocchio Feedback Filter\n",
    "# PROCESS 1 convert the raw lyrics into the concept space.\n",
    "# 1. Create a TFIDF vectorizer\n",
    "# 2. Create a document term matrix using TFIDF vec fit_transform using the raw lyrics. [I think there is an option to lemmatize here]\n",
    "# 3. Complete latent semantic indexing using TruncatedSVD(num components = num comncepts, specifiy the random state)\n",
    "# 4. Fit the document term matrix using TruncatedSVD.fit_transform. THESE ARE YOUR VECTORS FOR SIMILARITY SCORING\n",
    "\n",
    "# PROCESS 2 convert the query into a vector\n",
    "# 1. Convert querry into a raw string\n",
    "# 2. Use the TFIDF vectorizer above to transform the querry\n",
    "# 3. Use the LSI object above to convert the querry into the concept space.\n",
    "\n",
    "# PROCESS 3 execute the search\n",
    "# 1. Find the cosine similarity between the querry and all lyrics\n",
    "# 2. Sort the tracks by similarity\n",
    "# 3. Return the top N tracks to the user.\n",
    "\n",
    "# PROCESS 4 Rochhio Feedback Filtering\n",
    "# 1. Group user feeback by love, no answer, dislike\n",
    "# 2. Calculate the mean for each group\n",
    "# 3. Apply alpha, beta, gamma, and phi to:\n",
    "#       Original search, loves, hates, nuetral\n",
    "# 4. Update the lyric search querry vector and return new results!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning Parameters\n",
    "# minimum document frequency\n",
    "min_df = 10\n",
    "num_concepts = 100\n",
    "# step 1 import old lyrical data into a dataframe.\n",
    "lyric_df = pd.read_csv(practice_data_path)\n",
    "# drop latin songs to avoid negatively impacting performance using 2 languages\n",
    "lyric_df = lyric_df[lyric_df['genre'] != 'latin']\n",
    "lyric_df = lyric_df[['track_name_x', 'artist_name_x', 'track_id','lyric_raw']]\n",
    "lyric_df.rename(columns={\n",
    "    'track_name_x': 'track_name',\n",
    "    'artist_name_x': 'artist_name'\n",
    "}, inplace=True)\n",
    "lyric_df = lyric_df.dropna()\n",
    "# replace new line character\n",
    "lyric_df['lyric_raw'].replace('\\n', ' ',regex=True, inplace=True)\n",
    "# remove embed text from lyric genius API\n",
    "lyric_df['lyric_raw'].replace('[0-9]{1,3}Embed', '', regex=True, inplace=True)\n",
    "# create vectorizer object\n",
    "vecObj = TfidfVectorizer(tokenizer=str.split, min_df=min_df)\n",
    "# fit the TFIDF vectorizer\n",
    "docTermMat = vecObj.fit_transform(lyric_df['lyric_raw'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsiObj = TruncatedSVD(n_components=num_concepts, random_state=42)\n",
    "docVecs = lsiObj.fit_transform(docTermMat)\n",
    "# create a dataframe where the track id is the index the docVecs are the rows.\n",
    "track_vec_dict = defaultdict(list)\n",
    "track_ids = lyric_df['track_id'].values\n",
    "track_vec_dict = {track_ids[i]: docVecs[i] for i in range(len(track_ids))}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part 2 convert the query into a vector in the concept space\n",
    "user_query = 'Jealousy, turning saints into the sea Swimming through sick lullabies Choking on your alibis'\n",
    "# vectorize\n",
    "userVec = vecObj.transform([user_query])\n",
    "# convert query vec into the concept space\n",
    "userLsi = lsiObj.transform(userVec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part 3 execute search using cosine similarity\n",
    "# 1. Find the cosine similarity between the query and all lyrics\n",
    "# 2. Sort the tracks by similarity\n",
    "# 3. Return the top N tracks to the user.\n",
    "\n",
    "# calculate cosine similarity between every track and the lyric provided.\n",
    "simVals = cosine_similarity(docVecs, userLsi)\n",
    "# create a track name, track id, artist name, similarity dataframe\n",
    "lyric_df['similarity'] = simVals\n",
    "\n",
    "# this step is important, \n",
    "# the lyrics df is officially out of sync now, \n",
    "# the indexes need to be sorted again OR two copies need to be maintained\n",
    "sim_df = lyric_df.sort_values(by='similarity', ascending=False)\n",
    "user_playlist = sim_df.head(30)[['track_name', 'artist_name', 'track_id']]\n",
    "# initialize a feedback column and set every row to 0.\n",
    "user_playlist['feedback'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate user input here\n",
    "feedback = [np.random.randint(0,3) for i in range(len(user_playlist))]\n",
    "user_playlist['feedback'] = feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROCESS 4 Rochhio Feedback Filtering\n",
    "# 1. Group user feeback by love, nuetral, dislike\n",
    "# 2. Calculate the mean for each group\n",
    "# 3. Apply alpha, beta, gamma, and phi to:\n",
    "#       Original search, loves, hates, nuetral\n",
    "# 4. Update the lyric search querry vector and return new results!\n",
    "\n",
    "# tuning parameters\n",
    "# original querry gets no penalty\n",
    "alpha = 1.0\n",
    "# loved songs get a beta positive weight.\n",
    "beta = 0.75\n",
    "# disliked songs get a gamma negative weight\n",
    "gamma = 0.25\n",
    "# nuetral songs get a phi positive weight\n",
    "phi = 0.5\n",
    "\n",
    "# create a mean vector dict for all 3 states\n",
    "meanVectDict = defaultdict(list)\n",
    "\n",
    "# iterate through the three states nuetral[0], dislike[1], love[2]\n",
    "for i in range(3):\n",
    "    temp_tracks = user_playlist[user_playlist['feedback']==i]\n",
    "    if len(temp_tracks) > 0:\n",
    "        # this means that tracks with this sentiment exist.\n",
    "        # we can go get the track vectors from the track_vec_dict\n",
    "        tempVecs = [track_vec_dict[vec] for vec in temp_tracks['track_id']]\n",
    "        # next we need to calculate the mean vector for this segment\n",
    "        meanVec = np.mean(tempVecs, axis=0)\n",
    "        # add this mean to the mean vect dict. The key is the state.\n",
    "        meanVectDict[i] = meanVec\n",
    "    else:\n",
    "        # if there are no tracks in this state, set its mean to 0\n",
    "        meanVectDict[i] = 0\n",
    "\n",
    "# calcualte the new query vector by summing all of the mean vectors together\n",
    "newQueryVec = alpha*userLsi + beta * meanVectDict[2] - gamma * meanVectDict[1] + phi * meanVectDict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5 convert the notebook into functions.\n",
    "def load_lyrics_data():\n",
    "    \"\"\"\n",
    "    return a dataframe with cleaned lyrics\n",
    "    \"\"\"\n",
    "    temp_path = os.getcwd()\n",
    "    root_path = temp_path.split('/napster_2')[0]\n",
    "    repo_path = '/napster_2/lyric_genius_api/practice_data.csv'\n",
    "    data_path = root_path + repo_path\n",
    "    # step 1 import old lyrical data into a dataframe.\n",
    "    lyric_df = pd.read_csv(data_path)\n",
    "    # drop latin songs to avoid negatively impacting performance using 2 languages\n",
    "    lyric_df = lyric_df[lyric_df['genre'] != 'latin']\n",
    "    lyric_df = lyric_df[['track_name_x', 'artist_name_x', 'track_id','lyric_raw']]\n",
    "    lyric_df.rename(columns={\n",
    "        'track_name_x': 'track_name',\n",
    "        'artist_name_x': 'artist_name'\n",
    "    }, inplace=True)\n",
    "    lyric_df = lyric_df.dropna()\n",
    "    # replace new line character\n",
    "    lyric_df['lyric_raw'].replace('\\n', ' ',regex=True, inplace=True)\n",
    "    # remove embed text from lyric genius API\n",
    "    lyric_df['lyric_raw'].replace('[0-9]{1,3}Embed', '', regex=True, inplace=True)\n",
    "    return lyric_df\n",
    "\n",
    "def create_lyric_tfidf(lyric_df, min_df):\n",
    "    \"\"\" \n",
    "    Create a tfidf vectorizer for the track lyrics\n",
    "    \"\"\"\n",
    "    tfidf = TfidfVectorizer(tokenizer=str.split, min_df=min_df)\n",
    "    # fit the TFIDF vectorizer\n",
    "    tfidf.fit(lyric_df['raw_lyrics'])\n",
    "    return tfidf\n",
    "\n",
    "def lsi_lyrics(lyric_df, tfidf, num_concepts):\n",
    "    \"\"\"\n",
    "    fit an LSI object using the lyrics\n",
    "    \"\"\"\n",
    "    lyricTermMat = tfidf.transform(lyric_df['raw_lyrics'])\n",
    "    lsiObj = TruncatedSVD(n_components=num_concepts, random_state=15)\n",
    "    lsiObj.fit(lyricTermMat)\n",
    "    return lsiObj\n",
    "\n",
    "def create_lyric_vecs(lyric_df, lsiObj, tfidf):\n",
    "    \"\"\" \n",
    "    generate the lyric vectors in the concept space\n",
    "    \"\"\"\n",
    "    lyricTermMat = tfidf.transform(lyric_df['raw_lyrics'])\n",
    "    lyric_vecs = lsiObj.transform(lyricTermMat)\n",
    "    return lyric_vecs\n",
    "\n",
    "def create_lyric_dictionary(lyric_df, lyric_vecs):\n",
    "    \"\"\" \n",
    "    input a lyric dataframe and lsi lyric vectors\n",
    "    return a dictionary where the track id is the key\n",
    "    the vector is the value\n",
    "    \"\"\"\n",
    "    # create a dataframe where the track id is the index the docVecs are the rows.\n",
    "    track_vec_dict = defaultdict(list)\n",
    "    track_ids = lyric_df['track_id'].values\n",
    "    track_vec_dict = {track_ids[i]: lyric_vecs[i] for i in range(len(track_ids))}\n",
    "    return track_vec_dict\n",
    "\n",
    "def lsi_on_query(lsiObj, user_query, tfidf):\n",
    "    \"\"\" \n",
    "    Transform the user search string into the concept space\n",
    "    \"\"\"\n",
    "    userVec = tfidf.transform([user_query])\n",
    "    # convert query vec into the concept space\n",
    "    userLsi = lsiObj.transform(userVec)\n",
    "    return userLsi\n",
    "\n",
    "def retreive_30_tracks(lyric_df, userLsi, lyric_vecs):\n",
    "    \"\"\" \n",
    "    calculate cosine similarity between lyrics and user query\n",
    "    return the top 30 in a dataframe.\n",
    "    \"\"\"\n",
    "    # calculate cosine similarity between every track and the lyric provided.\n",
    "    simVals = cosine_similarity(lyric_vecs, userLsi)\n",
    "    # create a track name, track id, artist name, similarity dataframe\n",
    "    lyric_df['similarity'] = simVals\n",
    "    user_playlist = lyric_df.sort_values(by='similarity', ascending=False)\n",
    "    user_playlist = user_playlist.head(30)[['track_name', 'artist_name', 'track_id']]\n",
    "    # initialize a feedback column and set every row to 0.\n",
    "    user_playlist['feedback'] = 0\n",
    "    return user_playlist\n",
    "\n",
    "def simulate_user_input(user_playlist):\n",
    "    \"\"\" \n",
    "    Simulate user input assign 0,1,2 to user feedback\n",
    "    0 = nuetral\n",
    "    1 = dislike\n",
    "    2 = love\n",
    "    \"\"\"\n",
    "    feedback = [np.random.randint(0,3) for i in range(len(user_playlist))]\n",
    "    user_playlist['feedback'] = feedback\n",
    "    return user_playlist\n",
    "\n",
    "def rocchio_feedback(alpha, beta, gamma, phi, user_playlist, track_vec_dict):\n",
    "    \"\"\" \n",
    "    Rochhio Feedback Filtering\n",
    "    1. Group user feeback by love, nuetral, dislike\n",
    "    2. Calculate the mean for each group\n",
    "    3. Apply alpha, beta, gamma, and phi to:\n",
    "           Original search, loves, hates, nuetral\n",
    "    4. Update the lyric search querry vector and return new results!\n",
    "    return an updated query vector to improve search results\n",
    "    \"\"\"\n",
    "    # create a mean vector dict for all 3 states\n",
    "    meanVectDict = defaultdict(list)\n",
    "    # iterate through the three states nuetral[0], dislike[1], love[2]\n",
    "    for i in range(3):\n",
    "        temp_tracks = user_playlist[user_playlist['feedback']==i]\n",
    "        if len(temp_tracks) > 0:\n",
    "            # this means that tracks with this sentiment exist.\n",
    "            # we can go get the track vectors from the track_vec_dict\n",
    "            tempVecs = [track_vec_dict[vec] for vec in temp_tracks['track_id']]\n",
    "            # next we need to calculate the mean vector for this segment\n",
    "            meanVec = np.mean(tempVecs, axis=0)\n",
    "            # add this mean to the mean vect dict. The key is the state.\n",
    "            meanVectDict[i] = meanVec\n",
    "        else:\n",
    "            # if there are no tracks in this state, set its mean to 0\n",
    "            meanVectDict[i] = 0\n",
    "    # calcualte the new query vector by summing all of the mean vectors together\n",
    "    newQueryVec = alpha*userLsi + beta * meanVectDict[2] - gamma * meanVectDict[1] + phi * meanVectDict[0]\n",
    "    return newQueryVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cardoni/Library/Python/3.8/lib/python/site-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/var/folders/6r/kwmwk0wx4yj2ccxtwqdqgb4r0000gn/T/ipykernel_1463/690281095.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  lyric_df['similarity'] = simVals\n",
      "/var/folders/6r/kwmwk0wx4yj2ccxtwqdqgb4r0000gn/T/ipykernel_1463/690281095.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  lyric_df['similarity'] = simVals\n"
     ]
    }
   ],
   "source": [
    "# Step 6 integrate all functions into a loop\n",
    "user_query =  'Jealousy, turning saints into the sea Swimming through sick lullabies Choking on your alibis'\n",
    "# load in lyrics dataframe\n",
    "lyric_df = test_df #load_lyrics_data()\n",
    "# create a tfidf vectorizer object\n",
    "tfidf = create_lyric_tfidf(lyric_df, 10)\n",
    "# create a latent semantic indexing object\n",
    "lsiObj = lsi_lyrics(lyric_df, tfidf, 100)\n",
    "# convert the lyrics into content vectors\n",
    "lyric_vecs = create_lyric_vecs(lyric_df, lsiObj, tfidf)\n",
    "# create a dictionary mapping track id to content vector\n",
    "track_vec_dict = create_lyric_dictionary(lyric_df, lyric_vecs)\n",
    "# convert a user string query into the concept space\n",
    "userLsi = lsi_on_query(lsiObj, user_query, tfidf)\n",
    "# create a user playlist and return a dataframe\n",
    "user_playlist = retreive_30_tracks(lyric_df, userLsi, lyric_vecs)\n",
    "# simulate user input while we are not connected to the GUI\n",
    "user_playlist = simulate_user_input(user_playlist)\n",
    "# Apply rocchio feedback filter to generate a better query\n",
    "rocchioSearch = rocchio_feedback(1.0, 0.75, 0.25, 0.5, user_playlist, track_vec_dict)\n",
    "# generate a new user playlist with the updated search\n",
    "user_playlist = retreive_30_tracks(lyric_df, rocchioSearch, lyric_vecs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(track_vec_dict, open('lsi_vec_dict.p', 'wb'))\n",
    "pickle.dump(test_df, open('clean_english_tracks_df.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5edc29c2ed010d6458d71a83433b383a96a8cbd3efe8531bc90c4b8a5b8bcec9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
